{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Political Bias Detection with LSTM\n",
    "\n",
    "**Workshop Duration:** 45 minutes  \n",
    "**Dataset:** AllSides News Corpus (3 classes: Left, Center, Right)  \n",
    "**Expected Accuracy:** 65-80% (this is GOOD for bias detection!)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "- This is **harder** than topic classification\n",
    "- Bias is **subtle** and context-dependent\n",
    "- 70% accuracy is **good** (not 90%+)\n",
    "- **Read ETHICS.md** before deploying!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (NOTE: Different from Project 1!)\n",
    "MAX_WORDS = 20000        # Larger vocabulary (subtle words matter!)\n",
    "MAX_LEN = 400            # Longer sequences (need more context)\n",
    "EMBEDDING_DIM = 300      # Use 300d for better semantics\n",
    "LSTM_UNITS_1 = 128       # First BiLSTM layer\n",
    "LSTM_UNITS_2 = 64        # Second BiLSTM layer\n",
    "DROPOUT_RATE = 0.3       # LSTM dropout\n",
    "DENSE_DROPOUT = 0.5      # Dense dropout\n",
    "BATCH_SIZE = 32          # Smaller batch size\n",
    "EPOCHS = 15              # May need more epochs\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = 'data/allsides_news_complete.csv'\n",
    "GLOVE_PATH = 'embeddings/glove.840B.300d.txt'\n",
    "\n",
    "print(\"‚úì Configuration set!\")\n",
    "print(f\"NOTE: Using larger vocab ({MAX_WORDS}) and longer sequences ({MAX_LEN}) than Project 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Data Loading and Exploration\n",
    "\n",
    "**Goal:** Load AllSides dataset and understand class imbalance.\n",
    "\n",
    "**IMPORTANT:** Watch for class imbalance (fewer Center articles)!\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Load the AllSides dataset and analyze class imbalance\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load AllSides dataset\n",
    "# Hint: Column names might vary (check with df.columns)\n",
    "# Hint: Handle missing values\n",
    "# Hint: Ensure labels are integers (0, 1, 2)\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load AllSides bias dataset.\n",
    "    \n",
    "    Returns:\n",
    "        X: List of article texts\n",
    "        y: numpy array of labels (0=Left, 1=Center, 2=Right)\n",
    "    \"\"\"\n",
    "    # TODO: Implement data loading\n",
    "    pass\n",
    "\n",
    "# Load the data\n",
    "# X, y = load_data(DATA_PATH)\n",
    "# print(f\"‚úì Loaded {len(X)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore dataset with focus on imbalance\n",
    "# Hint: Show count AND percentage for each class\n",
    "# Hint: Calculate average length per bias\n",
    "# Hint: Show sample articles from each bias\n",
    "\n",
    "def explore_data(X, y):\n",
    "    \"\"\"\n",
    "    Explore dataset and identify imbalance.\n",
    "    \"\"\"\n",
    "    # TODO: Implement comprehensive exploration\n",
    "    pass\n",
    "\n",
    "# Explore the data\n",
    "# explore_data(X, y)\n",
    "# print(\"\\n‚ö†Ô∏è NOTE: If Center is underrepresented, we'll use class weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Part 2: Text Preprocessing\n",
    "\n",
    "**Goal:** Preprocess with larger vocabulary and longer sequences.\n",
    "\n",
    "**Key Differences from Project 1:**\n",
    "- Larger vocabulary (20,000 vs 10,000)\n",
    "- Longer sequences (400 vs 200)\n",
    "- **Stratified splitting** to maintain balance\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Preprocess for bias detection: vocab=20000, length=400, stratified split\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement preprocessing for bias detection\n",
    "# Hint: Use MAX_WORDS=20000 (larger vocab for subtle words)\n",
    "# Hint: Use MAX_LEN=400 (more context needed)\n",
    "# Hint: Use stratify parameter in train_test_split\n",
    "\n",
    "def preprocess_text(X, y, test_size=0.15, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Preprocess text with stratified splitting.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test: Padded sequences\n",
    "        y_train, y_val, y_test: Label arrays\n",
    "        tokenizer: Fitted tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Implement preprocessing\n",
    "    pass\n",
    "\n",
    "# Preprocess\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test, tokenizer = preprocess_text(X, y)\n",
    "# print(f\"‚úì Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "# print(f\"‚úì Vocabulary size: {len(tokenizer.word_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öñÔ∏è Part 3: Class Weights\n",
    "\n",
    "**Goal:** Compute class weights to handle imbalance.\n",
    "\n",
    "**Why:** Center articles are fewer, so we give them higher weight during training.\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Calculate class weights to handle the imbalanced dataset\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute class weights\n",
    "# Hint: Use sklearn's compute_class_weight\n",
    "# Hint: Convert to dict format for Keras\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    \"\"\"\n",
    "    Compute class weights for imbalanced data.\n",
    "    \n",
    "    Returns:\n",
    "        class_weights: Dictionary {0: weight0, 1: weight1, 2: weight2}\n",
    "    \"\"\"\n",
    "    # TODO: Implement class weight calculation\n",
    "    pass\n",
    "\n",
    "# Compute weights\n",
    "# class_weights = compute_class_weights(y_train)\n",
    "# print(\"Class weights:\")\n",
    "# for cls, weight in class_weights.items():\n",
    "#     print(f\"  Class {cls}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 4: GloVe Embeddings (300d)\n",
    "\n",
    "**Goal:** Load GloVe 300d for better semantic understanding.\n",
    "\n",
    "**NOTE:** This is **strongly recommended** (not optional) for bias detection!\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Load GloVe 300d embeddings and create embedding matrix\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load GloVe 300d (WARNING: ~2GB, takes 1-2 minutes)\n",
    "# Hint: Print progress every 100k words\n",
    "\n",
    "def load_glove_embeddings(filepath):\n",
    "    \"\"\"\n",
    "    Load GloVe 300d embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings_index: Dictionary word -> vector\n",
    "    \"\"\"\n",
    "    # TODO: Implement GloVe loading\n",
    "    pass\n",
    "\n",
    "# Load embeddings\n",
    "# print(\"Loading GloVe 300d (be patient, ~2GB file)...\")\n",
    "# embeddings_index = load_glove_embeddings(GLOVE_PATH)\n",
    "# print(f\"‚úì Loaded {len(embeddings_index)} word embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embedding matrix\n",
    "# Hint: Track coverage (% of vocab found in GloVe)\n",
    "\n",
    "def create_embedding_matrix(word_index, embeddings_index):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: numpy array (vocab_size, 300)\n",
    "    \"\"\"\n",
    "    # TODO: Implement embedding matrix creation\n",
    "    pass\n",
    "\n",
    "# Create matrix\n",
    "# embedding_matrix = create_embedding_matrix(tokenizer.word_index, embeddings_index)\n",
    "# print(f\"‚úì Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Part 5: Stacked Bidirectional LSTM\n",
    "\n",
    "**Goal:** Build more complex model than Project 1.\n",
    "\n",
    "**Architecture:**\n",
    "1. Embedding (300d, GloVe)\n",
    "2. Bidirectional LSTM 1 (128 units, return sequences)\n",
    "3. Bidirectional LSTM 2 (64 units)\n",
    "4. Dense (64 units, ReLU, dropout 0.5)\n",
    "5. Output (3 classes, softmax)\n",
    "\n",
    "**Why Bidirectional?** Context from both directions helps detect bias.\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Build stacked bidirectional LSTM for bias detection\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build stacked bidirectional LSTM\n",
    "# Hint: Use Bidirectional(LSTM(...)) for both LSTM layers\n",
    "# Hint: First LSTM needs return_sequences=True\n",
    "# Hint: Use dropout in LSTM layers AND dense layer\n",
    "\n",
    "def build_model(vocab_size, embedding_matrix=None):\n",
    "    \"\"\"\n",
    "    Build stacked bidirectional LSTM.\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # TODO: Implement model building\n",
    "    pass\n",
    "\n",
    "# Build model\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# model = build_model(vocab_size, embedding_matrix)\n",
    "# model.summary()\n",
    "# print(\"\\nNOTE: This model has ~2-3M parameters (more complex than Project 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 6: Training with Class Weights\n",
    "\n",
    "**Goal:** Train model while handling class imbalance.\n",
    "\n",
    "**Expectations:**\n",
    "- Training will be **slower** (bidirectional LSTMs)\n",
    "- Accuracy will be **lower** (65-80% is good!)\n",
    "- Watch for **overfitting**\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Train with class weights and early stopping. Monitor for overfitting.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with class weights\n",
    "# Hint: Pass class_weight to model.fit()\n",
    "# Hint: Use EarlyStopping with patience=5\n",
    "# Hint: Use ReduceLROnPlateau and ModelCheckpoint\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, class_weights=None):\n",
    "    \"\"\"\n",
    "    Train bidirectional LSTM.\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    # TODO: Implement training\n",
    "    pass\n",
    "\n",
    "# Train\n",
    "# print(\"Training (this will take longer than Project 1)...\")\n",
    "# history = train_model(model, X_train, y_train, X_val, y_val, class_weights)\n",
    "# print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Part 7: Visualization\n",
    "\n",
    "**Goal:** Plot training curves and watch for overfitting.\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Plot training curves. Is the model overfitting?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training curves.\n",
    "    \"\"\"\n",
    "    # TODO: Implement plotting\n",
    "    pass\n",
    "\n",
    "# Plot\n",
    "# plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 8: Comprehensive Evaluation\n",
    "\n",
    "**Goal:** Evaluate with focus on per-class performance.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Is Center class performing poorly?\n",
    "- Which biases get confused (Left‚ÜîCenter vs Right‚ÜîCenter)?\n",
    "- Is 70% accuracy acceptable?\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Evaluate model and analyze per-class performance. Which bias is hardest?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprehensive evaluation\n",
    "# Hint: Show overall AND per-class metrics\n",
    "# Hint: Generate confusion matrix with labels\n",
    "# Hint: Discuss which biases are confused\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate with focus on per-class metrics.\n",
    "    \"\"\"\n",
    "    # TODO: Implement evaluation\n",
    "    pass\n",
    "\n",
    "# Evaluate\n",
    "# evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Part 9: Real-World Testing\n",
    "\n",
    "**Goal:** Test on articles from known sources.\n",
    "\n",
    "**Claude Code Prompt:**  \n",
    "`\"Test on articles from CNN, Fox News, BBC. Does it match known source bias?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement prediction function\n",
    "\n",
    "def predict_bias(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict bias of article.\n",
    "    \n",
    "    Returns:\n",
    "        bias: String ('Left', 'Center', 'Right')\n",
    "        confidence: Probability\n",
    "        all_probs: All class probabilities\n",
    "    \"\"\"\n",
    "    # TODO: Implement prediction\n",
    "    pass\n",
    "\n",
    "# Test on real articles\n",
    "bias_names = ['Left', 'Center', 'Right']\n",
    "\n",
    "# TODO: Add real article texts here\n",
    "test_articles = {\n",
    "    \"CNN (Left-leaning)\": \"[Add real article text]\",\n",
    "    \"Fox News (Right-leaning)\": \"[Add real article text]\",\n",
    "    \"BBC (Center)\": \"[Add real article text]\",\n",
    "}\n",
    "\n",
    "# TODO: Test predictions\n",
    "# for source, article in test_articles.items():\n",
    "#     bias, conf, probs = predict_bias(article, model, tokenizer)\n",
    "#     print(f\"Source: {source}\")\n",
    "#     print(f\"Predicted: {bias} ({conf:.2%})\")\n",
    "#     print(f\"All probabilities: Left={probs[0]:.2%}, Center={probs[1]:.2%}, Right={probs[2]:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí≠ Part 10: Ethical Discussion\n",
    "\n",
    "**CRITICAL:** Before deploying, discuss these questions:\n",
    "\n",
    "### Questions to Consider:\n",
    "\n",
    "1. **What is bias?**\n",
    "   - Is \"center\" objectively definable?\n",
    "   - Does bias = incorrect?\n",
    "\n",
    "2. **Model limitations:**\n",
    "   - 70% accuracy means 30% errors\n",
    "   - Model sees patterns, not truth\n",
    "   - Context matters\n",
    "\n",
    "3. **Potential harms:**\n",
    "   - Could this be used for censorship?\n",
    "   - Might it increase polarization?\n",
    "   - What about false positives?\n",
    "\n",
    "4. **Responsible use:**\n",
    "   - Education: YES ‚úÖ\n",
    "   - Content moderation: Careful ‚ö†Ô∏è\n",
    "   - Automated filtering: NO ‚ùå\n",
    "\n",
    "**Read ETHICS.md for full discussion!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've built a bias detector using advanced LSTMs! üöÄ\n",
    "\n",
    "### Key Takeaways:\n",
    "1. ‚úÖ Bias detection is **harder** than topic classification\n",
    "2. ‚úÖ 70% accuracy is **good** for this task\n",
    "3. ‚úÖ Bidirectional LSTMs capture **context**\n",
    "4. ‚úÖ Class imbalance needs **special handling**\n",
    "5. ‚úÖ **Ethics matter** - use responsibly!\n",
    "\n",
    "### Next Steps:\n",
    "1. Compare with BERT/transformers\n",
    "2. Add attention mechanism\n",
    "3. Build explainability tools (LIME/SHAP)\n",
    "4. Create web interface\n",
    "5. **Most importantly:** Read ETHICS.md!\n",
    "\n",
    "### Save Your Model:\n",
    "```python\n",
    "model.save('bias_detector_bilstm.h5')\n",
    "```\n",
    "\n",
    "### Convert to Python Script:\n",
    "```bash\n",
    "jupyter nbconvert --to script starter_notebook.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Remember: Build responsibly! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
